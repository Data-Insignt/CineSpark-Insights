{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Basic Data Manipulation & Simple Recommendation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Read in the rating file and create an RDD consisting of parsed lines, then count the number of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a Spark session\n",
    "sc = SparkContext .getOrCreate()\n",
    "\n",
    "ratings_rdd = sc.textFile(\"dataset/ratings.csv\")\n",
    "header = ratings_rdd.first()\n",
    "\n",
    "# Remove the header and then parse each line\n",
    "ratings_rdd = ratings_rdd.filter(lambda line: line != header) \\\n",
    "    .map(lambda line: line.split(',')) \\\n",
    "    .map(lambda tokens: (tokens[0], tokens[1], float(tokens[2]), tokens[3]))\n",
    "\n",
    "num_ratings = ratings_rdd.count()\n",
    "print(num_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recommend 5 movies with the highest average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MovieLens\").config(\"spark.executor.memory\", \"3g\").config(\"spark.driver.memory\", \"2g\").getOrCreate()\n",
    "ratings_df = spark.createDataFrame(ratings_rdd, [\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
    "\n",
    "#  Count the number of ratings\n",
    "avg_ratings_df = ratings_df.groupBy(\"movieId\").avg(\"rating\")\n",
    "\n",
    "# Recommend 5 movies with the highest average rating\n",
    "top_movies = avg_ratings_df.orderBy(\"avg(rating)\", ascending=False).limit(5)\n",
    "\n",
    "top_movies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Other operations to enrich your data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Try to create visualizations to convey the insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve the data in the form of a Pandas DataFrame\n",
    "top_movies_pd = top_movies.toPandas()\n",
    "\n",
    "# Create a visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_movies_pd['movieId'], top_movies_pd['avg(rating)'])\n",
    "plt.xlabel('Movie ID')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Top 5 Movies by Average Rating')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First split rating data into 70% training set and 30% testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "ratings_ml = ratings_df.rdd.map(lambda r: Row(userId=int(r[0]), movieId=int(r[1]), rating=float(r[2])))\n",
    "ratings_ml_df = spark.createDataFrame(ratings_ml)\n",
    "\n",
    "# Split the dataset\n",
    "(training, test) = ratings_ml_df.randomSplit([0.1, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose one matrix factorization algorithm to predict the rating score based on the rating data file only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "\n",
    "# Create ALS model\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\", nonnegative=True)\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate top 3 recommendations for all users using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = model.recommendForAllUsers(3)\n",
    "recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract features from movies and users (join movie and user data and do some feature transformation), then build another machine learning model to predict rating scores for the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read and integrate additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/root/CineSpark-Insights/ex_laurie.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/ex_laurie.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m indexed \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtransform(movies_df)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/ex_laurie.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m encoder \u001b[39m=\u001b[39m OneHotEncoder(inputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgenresIndex\u001b[39m\u001b[39m\"\u001b[39m, outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgenresVec\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/ex_laurie.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m movies_encoded \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mtransform(indexed)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "# Read additional data\n",
    "tags_df = spark.read.csv(\"dataset/tags.csv\", header=True, inferSchema=True)\n",
    "movies_df = spark.read.csv(\"dataset/movies.csv\", header=True, inferSchema=True)\n",
    "genome_scores_df = spark.read.csv(\"dataset/genome-scores.csv\", header=True, inferSchema=True)\n",
    "genome_tags_df = spark.read.csv(\"dataset/genome-tags.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Potential feature transformations\n",
    "# For example, perform one-hot encoding on movie genres\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Type conversion\n",
    "stringIndexer = StringIndexer(inputCol=\"genres\", outputCol=\"genresIndex\")\n",
    "model = stringIndexer.fit(movies_df)\n",
    "indexed = model.transform(movies_df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"genresIndex\", outputCol=\"genresVec\")\n",
    "movies_encoded = encoder.transform(indexed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并标签和评分数据\n",
    "# 这里是一个简化的例子，具体实现可能更复杂\n",
    "tag_features_df = tags_df.join(genome_scores_df, \"movieId\").join(genome_tags_df, \"tagId\")\n",
    "\n",
    "# 将电影信息和标签特征合并\n",
    "movie_features_df = movies_encoded.join(tag_features_df, \"movieId\")\n",
    "\n",
    "# 合并用户评分和电影特征\n",
    "complete_data_df = ratings_ml_df.join(movie_features_df, \"movieId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build and train machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# 特征向量化\n",
    "assembler = VectorAssembler(inputCols=[\"genresVec\", \"tagFeatures\"], outputCol=\"features\")\n",
    "data_ready = assembler.transform(complete_data_df)\n",
    "\n",
    "# 划分数据集\n",
    "(training_features, test_features) = data_ready.randomSplit([0.7, 0.3])\n",
    "\n",
    "# 使用随机森林模型\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"rating\")\n",
    "rf_model = rf.fit(training_features)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions_rf = rf_model.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare the pros and cons of these two models and report it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALS模型:\n",
    "+ 优点：适合大规模数据集，能有效处理稀疏性问题，常用于推荐系统。\n",
    "+ 缺点：需要调整多个参数，对冷启动问题敏感。\n",
    "\n",
    "随机森林模型:\n",
    "+ 优点：处理非线性关系效果好，不太容易过拟合。\n",
    "+ 缺点：需要大量特征工程，计算成本较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try to create visualizations to convey the insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算误差\n",
    "predictions_pd = predictions.toPandas()\n",
    "predictions_rf_pd = predictions_rf.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(predictions_pd['rating'] - predictions_pd['prediction'], bins=20, color='blue', alpha=0.7)\n",
    "plt.title('ALS Prediction Error')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions_rf_pd['rating'] - predictions_rf_pd['prediction'], bins=20, color='green', alpha=0.7)\n",
    "plt.title('Random Forest Prediction Error')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
