version: "2"
services:
  # Hadoop services
  namenode:
    user: "root"
    image: apache/hadoop:3
    hostname: namenode
    container_name: namenode
    command: /bin/bash -c "/bin/chmod -R 777 /opt/hadoop/etc/hadoop && mkdir -p /opt/hadoop/dfs/name && hdfs namenode -format -force && hdfs namenode"
    ports:
      - "9870:9870"
    env_file:
      - hadoop.env
      - config
    volumes:
      - /root/hadoop-cluster/hadoop-config:/opt/hadoop/etc/hadoop

  datanode1:
    image: apache/hadoop:3
    hostname: datanode1
    container_name: datanode1
    command: [ "hdfs", "datanode" ]
    env_file:
      - hadoop.env
      - config

  datanode2:
    image: apache/hadoop:3
    hostname: datanode2
    container_name: datanode2
    command: [ "hdfs", "datanode" ]
    env_file:
      - hadoop.env
      - config

  resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager
    container_name: resourcemanager
    command: [ "yarn", "resourcemanager" ]
    ports:
      - "8088:8088"
    env_file:
      - hadoop.env
      - config

  nodemanager1:
    image: apache/hadoop:3
    hostname: nodemanager1
    container_name: nodemanager1
    command: [ "yarn", "nodemanager" ]
    env_file:
      - hadoop.env
      - config

  nodemanager2:
    image: apache/hadoop:3
    hostname: nodemanager2
    container_name: nodemanager2
    command: [ "yarn", "nodemanager" ]
    env_file:
      - hadoop.env
      - config


  # Spark services
  spark-master:
    image: apache/spark:3.3.1
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - /root/hadoop-cluster/hadoop-config:/opt/hadoop/conf
    command: /bin/bash -c "/opt/spark/bin/start-master.sh"

  spark-worker:
    image: apache/spark:3.3.1
    volumes:
      - /root/hadoop-cluster/hadoop-config:/opt/hadoop/conf
    command: /bin/bash -c "/opt/spark/bin/start-worker.sh spark://spark-master:7077"
    depends_on:
      - spark-master