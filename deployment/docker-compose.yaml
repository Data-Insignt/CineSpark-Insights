version: "2"
services:
  namenode:
    user: "root"
    image: apache/hadoop:3
    hostname: namenode
    container_name: namenode
    command: /bin/bash -c "/bin/chmod -R 777 /opt/hadoop/etc/hadoop && mkdir -p /opt/hadoop/dfs/name && hdfs namenode -format -force && hdfs namenode"
    ports:
      - "9870:9870"
    env_file:
      - hadoop.env
      - config
    volumes:
      - /root/hadoop-cluster/hadoop-config:/opt/hadoop/etc/hadoop

  datanode1:
    image: apache/hadoop:3
    hostname: datanode1
    container_name: datanode1
    command: [ "hdfs", "datanode" ]
    env_file:
      - hadoop.env
      - config

  datanode2:
    image: apache/hadoop:3
    hostname: datanode2
    container_name: datanode2
    command: [ "hdfs", "datanode" ]
    env_file:
      - hadoop.env
      - config

  resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager
    container_name: resourcemanager
    command: [ "yarn", "resourcemanager" ]
    ports:
      - "8088:8088"
    env_file:
      - hadoop.env
      - config

  nodemanager1:
    image: apache/hadoop:3
    hostname: nodemanager1
    container_name: nodemanager1
    command: [ "yarn", "nodemanager" ]
    env_file:
      - hadoop.env
      - config

  nodemanager2:
    image: apache/hadoop:3
    hostname: nodemanager2
    container_name: nodemanager2
    command: [ "yarn", "nodemanager" ]
    env_file:
      - hadoop.env
      - config

  spark-master:
    image: apache/spark:3.3.1
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - /root/hadoop-cluster/spark-config:/opt/spark/conf
      - /root/hadoop-cluster/hadoop-config:/opt/hadoop/conf
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      SPARK_HOME: "/opt/spark"
      HADOOP_CONF_DIR: "/opt/hadoop/conf"
      YARN_CONF_DIR: "/opt/hadoop/conf"

  spark-worker:
    image: apache/spark:3.3.1
    container_name: spark-worker
    volumes:
      - /root/hadoop-cluster/spark-config:/opt/spark/conf
      - /root/hadoop-cluster/hadoop-config:/opt/hadoop/conf
      - /root/hadoop-cluster/spark-worker-data:/opt/spark/work
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_HOME: "/opt/spark"
      HADOOP_CONF_DIR: "/opt/hadoop/conf"
      YARN_CONF_DIR: "/opt/hadoop/conf"