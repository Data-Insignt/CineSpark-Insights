{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 1: Basic Data Manipulation & Simple Recommendation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.Read in the rating file and create an RDD consisting of parsed lines, then count the number of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:13.970800Z",
     "start_time": "2023-11-18T04:43:57.362997Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 09:34:32 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.102.123 instead (on interface ens192)\n",
      "23/11/23 09:34:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/23 09:34:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/23 09:34:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/11/23 09:34:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "[Stage 1:======================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a Spark session\n",
    "sc = SparkContext .getOrCreate()\n",
    "\n",
    "ratings_rdd = sc.textFile(\"dataset/ratings.csv\")\n",
    "header = ratings_rdd.first()\n",
    "\n",
    "# Remove the header and then parse each line\n",
    "ratings_rdd = ratings_rdd.filter(lambda line: line != header) \\\n",
    "    .map(lambda line: line.split(',')) \\\n",
    "    .map(lambda tokens: (tokens[0], tokens[1], float(tokens[2]), tokens[3]))\n",
    "\n",
    "num_ratings = ratings_rdd.count()\n",
    "print(num_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Recommend 5 movies with the highest average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:48.683341Z",
     "start_time": "2023-11-18T04:44:16.313299Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|movieId|avg(rating)|\n",
      "+-------+-----------+\n",
      "| 169820|        5.0|\n",
      "| 182345|        5.0|\n",
      "| 195641|        5.0|\n",
      "| 193529|        5.0|\n",
      "| 140014|        5.0|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MovieLens\").config(\"spark.driver.memory\", \"4G\").\\\n",
    "  config(\"spark.executor.memory\", \"1.5G\").getOrCreate()\n",
    "ratings_df = spark.createDataFrame(ratings_rdd, [\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
    "\n",
    "#  Count the number of ratings\n",
    "avg_ratings_df = ratings_df.groupBy(\"movieId\").avg(\"rating\")\n",
    "\n",
    "# Recommend 5 movies with the highest average rating\n",
    "top_movies = avg_ratings_df.orderBy(\"avg(rating)\", ascending=False).limit(5)\n",
    "\n",
    "top_movies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Other operations to enrich your data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:48.688111Z",
     "start_time": "2023-11-18T04:44:48.686403Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Try to create visualizations to convey the insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part2: Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:26:27.728586Z",
     "start_time": "2023-11-18T04:26:27.034583Z"
    },
    "collapsed": false
   },
   "source": [
    "## 1. First split rating data into 70% training set and 30% testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:24.738544Z",
     "start_time": "2023-11-18T04:45:20.115597Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 09:35:57 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 6 (TID 52): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a Spark session\n",
    "sc = SparkContext .getOrCreate()\n",
    "\n",
    "ratings_rdd = sc.textFile(\"dataset/ratings.csv\")\n",
    "header = ratings_rdd.first()\n",
    "\n",
    "# Remove the header and then parse each line\n",
    "ratings_rdd = ratings_rdd.filter(lambda line: line != header) \\\n",
    "    .map(lambda line: line.split(',')) \\\n",
    "    .map(lambda tokens: (tokens[0], tokens[1], float(tokens[2]), tokens[3]))\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MovieLens\").config(\"spark.driver.memory\", \"4G\").\\\n",
    "  config(\"spark.executor.memory\", \"1.5G\").getOrCreate()\n",
    "ratings_df = spark.createDataFrame(ratings_rdd, [\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "ratings_ml = ratings_df.rdd.map(lambda r: Row(userId=int(r[0]), movieId=int(r[1]), rating=float(r[2])))\n",
    "ratings_ml_df = spark.createDataFrame(ratings_ml)\n",
    "\n",
    "# Split the dataset\n",
    "(training, test) = ratings_ml_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Choose one matrix factorization algorithm to predict the rating score based on the rating data file only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 09:37:38 WARN BlockManager: Block rdd_38_5 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:38 WARN BlockManager: Block rdd_39_5 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:38 ERROR Executor: Exception in task 5.0 in stage 10.0 (TID 101)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockSort.allocate(ALS.scala:1596)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockSort.allocate(ALS.scala:1545)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.ensureCapacity(TimSort.java:943)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeLo(TimSort.java:691)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeAt(TimSort.java:517)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeCollapse(TimSort.java:445)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.access$200(TimSort.java:308)\n",
      "\tat org.apache.spark.util.collection.TimSort.sort(TimSort.java:136)\n",
      "\tat org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlock.org$apache$spark$ml$recommendation$ALS$UncompressedInBlock$$sort(ALS.scala:1516)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlock.compress(ALS.scala:1476)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3645/2048962244.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3627/279566245.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1222024267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3595/272226873.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1222024267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "23/11/23 09:37:39 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 10.0 (TID 101),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockSort.allocate(ALS.scala:1596)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockSort.allocate(ALS.scala:1545)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.ensureCapacity(TimSort.java:943)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeLo(TimSort.java:691)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeAt(TimSort.java:517)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeCollapse(TimSort.java:445)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.access$200(TimSort.java:308)\n",
      "\tat org.apache.spark.util.collection.TimSort.sort(TimSort.java:136)\n",
      "\tat org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlock.org$apache$spark$ml$recommendation$ALS$UncompressedInBlock$$sort(ALS.scala:1516)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlock.compress(ALS.scala:1476)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3645/2048962244.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3627/279566245.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1222024267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3595/272226873.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1222024267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "23/11/23 09:37:39 WARN TaskSetManager: Lost task 5.0 in stage 10.0 (TID 101) (192.168.102.123 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockSort.allocate(ALS.scala:1596)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockSort.allocate(ALS.scala:1545)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.ensureCapacity(TimSort.java:943)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeLo(TimSort.java:691)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeAt(TimSort.java:517)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeCollapse(TimSort.java:445)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.access$200(TimSort.java:308)\n",
      "\tat org.apache.spark.util.collection.TimSort.sort(TimSort.java:136)\n",
      "\tat org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlock.org$apache$spark$ml$recommendation$ALS$UncompressedInBlock$$sort(ALS.scala:1516)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlock.compress(ALS.scala:1476)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3645/2048962244.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3627/279566245.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1222024267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3595/272226873.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1222024267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\n",
      "23/11/23 09:37:39 ERROR TaskSetManager: Task 5 in stage 10.0 failed 1 times; aborting job\n",
      "23/11/23 09:37:39 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 10.0 failed 1 times, most recent failure: Lost task 5.0 in stage 10.0 (TID 101) (192.168.102.123 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockSort.allocate(ALS.scala:1596)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockSort.allocate(ALS.scala:1545)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.ensureCapacity(TimSort.java:943)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeLo(TimSort.java:691)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeAt(TimSort.java:517)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeCollapse(TimSort.java:445)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.access$200(TimSort.java:308)\n",
      "\tat org.apache.spark.util.collection.TimSort.sort(TimSort.java:136)\n",
      "\tat org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlock.org$apache$spark$ml$recommendation$ALS$UncompressedInBlock$$sort(ALS.scala:1516)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlock.compress(ALS.scala:1476)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3645/2048962244.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3627/279566245.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1222024267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3595/272226873.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1222024267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1266)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:988)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:737)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:714)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:616)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockSort.allocate(ALS.scala:1596)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockSort.allocate(ALS.scala:1545)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.ensureCapacity(TimSort.java:943)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeLo(TimSort.java:691)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeAt(TimSort.java:517)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.mergeCollapse(TimSort.java:445)\n",
      "\tat org.apache.spark.util.collection.TimSort$SortState.access$200(TimSort.java:308)\n",
      "\tat org.apache.spark.util.collection.TimSort.sort(TimSort.java:136)\n",
      "\tat org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlock.org$apache$spark$ml$recommendation$ALS$UncompressedInBlock$$sort(ALS.scala:1516)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlock.compress(ALS.scala:1476)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3645/2048962244.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3627/279566245.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1222024267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3595/272226873.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1222024267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_38_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_38_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_38_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_38_0 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_38_3 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_38_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_38_7 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_38_1 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_38_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_38_4 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_38_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_38_2 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_39_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_39_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_39_3 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_39_0 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_39_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_39_7 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_39_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_38_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_39_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_39_2 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_38_6 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_39_4 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_39_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_39_6 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_39_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_39_1 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 96) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 09:37:39 WARN TaskSetManager: Lost task 7.0 in stage 10.0 (TID 103) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 09:37:39 WARN TaskSetManager: Lost task 2.0 in stage 10.0 (TID 98) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 09:37:39 WARN TaskSetManager: Lost task 6.0 in stage 10.0 (TID 102) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 09:37:39 WARN TaskSetManager: Lost task 3.0 in stage 10.0 (TID 99) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 09:37:39 WARN TaskSetManager: Lost task 4.0 in stage 10.0 (TID 100) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 09:37:39 WARN TaskSetManager: Lost task 1.0 in stage 10.0 (TID 97) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_38_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_38_8 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 09:37:39 WARN BlockManager: Putting block rdd_39_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 09:37:39 WARN BlockManager: Block rdd_39_8 could not be removed as it was not found on disk or in memory\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 51136)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_30276/936185033.py\", line 29, in <module>\n",
      "    model = als.fit(training)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m/root/CineSpark-Insights/text_lgf.ipynb 单元格 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/text_lgf.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Fit ALS model on training data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/text_lgf.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m model \u001b[39m=\u001b[39m als\u001b[39m.\u001b[39;49mfit(training)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/text_lgf.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Evaluate on test data\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2116\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     traceback\u001b[39m.\u001b[39mprint_exc()\n\u001b[1;32m   2114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2116\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_showtraceback(etype, value, stb)\n\u001b[1;32m   2117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_pdb:\n\u001b[1;32m   2118\u001b[0m     \u001b[39m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebugger(force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/ipykernel/zmqshell.py:556\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    550\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    551\u001b[0m sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    553\u001b[0m exc_content \u001b[39m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtraceback\u001b[39m\u001b[39m\"\u001b[39m: stb,\n\u001b[1;32m    555\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mename\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(etype\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m),\n\u001b[0;32m--> 556\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevalue\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39;49m(evalue),\n\u001b[1;32m    557\u001b[0m }\n\u001b[1;32m    559\u001b[0m dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayhook\n\u001b[1;32m    560\u001b[0m \u001b[39m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[39m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_exception\u001b[39m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[39m=\u001b[39m gateway_client\u001b[39m.\u001b[39;49msend_command(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[39m=\u001b[39m get_return_value(answer, gateway_client, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[39m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[39m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[39m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "\n",
    "# Create ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\", nonnegative=True)\n",
    "\n",
    "# Define a grid of parameters to search\n",
    "param_grid = {\n",
    "    \"rank\": [10, 20, 30],\n",
    "    \"maxIter\": [5, 10, 20],\n",
    "    \"regParam\": [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "best_model = None\n",
    "best_error = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Grid search through parameters\n",
    "for rank in param_grid[\"rank\"]:\n",
    "    for max_iter in param_grid[\"maxIter\"]:\n",
    "        for reg_param in param_grid[\"regParam\"]:\n",
    "            als.setParams(rank=rank, maxIter=max_iter, regParam=reg_param)\n",
    "            \n",
    "            # Fit ALS model on training data\n",
    "            model = als.fit(training)\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            predictions = model.transform(test)\n",
    "            error = evaluator.evaluate(predictions)\n",
    "            \n",
    "            # Update best parameters if current model is better\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_model = model\n",
    "                best_params = {\"rank\": rank, \"maxIter\": max_iter, \"regParam\": reg_param}\n",
    "\n",
    "# Output the best parameters and error\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best RMSE:\", best_error)\n",
    "\n",
    "# Make predictions using the best model\n",
    "test_predictions = best_model.transform(test)\n",
    "\n",
    "# Generate top 3 recommendations for all users using the best model\n",
    "recommendations = best_model.recommendForAllUsers(3)\n",
    "recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on the test data = {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Extract features from movies and users (join movie and user data and do some feature transformation), then build another machine learning model to predict rating scores for the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.1 Read and integrate additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read additional data\n",
    "tags_df = spark.read.csv(\"dataset/tags.csv\", header=True, inferSchema=True)\n",
    "movies_df = spark.read.csv(\"dataset/movies.csv\", header=True, inferSchema=True)\n",
    "genome_scores_df = spark.read.csv(\"dataset/genome-scores.csv\", header=True, inferSchema=True)\n",
    "genome_tags_df = spark.read.csv(\"dataset/genome-tags.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Potential feature transformations\n",
    "# For example, perform one-hot encoding on movie genres\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Type conversion\n",
    "stringIndexer = StringIndexer(inputCol=\"genres\", outputCol=\"genresIndex\")\n",
    "model = stringIndexer.fit(movies_df)\n",
    "indexed = model.transform(movies_df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"genresIndex\", outputCol=\"genresVec\")\n",
    "movies_encoded = encoder.transform(indexed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 合并标签和评分数据\n",
    "# 这里是一个简化的例子，具体实现可能更复杂\n",
    "tag_features_df = tags_df.join(genome_scores_df, \"movieId\").join(genome_tags_df, \"tagId\")\n",
    "\n",
    "# 将电影信息和标签特征合并\n",
    "movie_features_df = movies_encoded.join(tag_features_df, \"movieId\")\n",
    "\n",
    "# 合并用户评分和电影特征\n",
    "complete_data_df = ratings_ml_df.join(movie_features_df, \"movieId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 Build and train machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# 特征向量化\n",
    "assembler = VectorAssembler(inputCols=[\"genresVec\", \"tagFeatures\"], outputCol=\"features\")\n",
    "data_ready = assembler.transform(complete_data_df)\n",
    "\n",
    "# 划分数据集\n",
    "(training_features, test_features) = data_ready.randomSplit([0.7, 0.3])\n",
    "\n",
    "# 使用随机森林模型\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"rating\")\n",
    "rf_model = rf.fit(training_features)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions_rf = rf_model.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Compare the pros and cons of these two models and report it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ALS模型:\n",
    "+ 优点：适合大规模数据集，能有效处理稀疏性问题，常用于推荐系统。\n",
    "+ 缺点：需要调整多个参数，对冷启动问题敏感。\n",
    "\n",
    "随机森林模型:\n",
    "+ 优点：处理非线性关系效果好，不太容易过拟合。\n",
    "+ 缺点：需要大量特征工程，计算成本较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. Try to create visualizations to convey the insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算误差\n",
    "predictions_pd = predictions.toPandas()\n",
    "predictions_rf_pd = predictions_rf.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(predictions_pd['rating'] - predictions_pd['prediction'], bins=20, color='blue', alpha=0.7)\n",
    "plt.title('ALS Prediction Error')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions_rf_pd['rating'] - predictions_rf_pd['prediction'], bins=20, color='green', alpha=0.7)\n",
    "plt.title('Random Forest Prediction Error')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
