{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Environment Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, collect_list, split, explode\n",
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Init Spark Session with Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:13.970800Z",
     "start_time": "2023-11-18T04:43:57.362997Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_spark_session_with_ratings(filename):\n",
    "    \"\"\"\n",
    "    Reads the ratings file and returns the SparkSession and DataFrame created from the file.\n",
    "\n",
    "    Args:\n",
    "    filename (str): The path to the ratings file.\n",
    "\n",
    "    Returns:\n",
    "    SparkSession, DataFrame: The Spark session and the DataFrame created from the ratings file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MovieRecommend\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.executor.instances\", \"4\") \\\n",
    "        .config(\"spark.driver.memory\", \"8G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Retrieve SparkContext from SparkSession\n",
    "    sc = spark.sparkContext\n",
    "    # sc.setLogLevel(\"INFO\")\n",
    "\n",
    "    # Load the data into an RDD\n",
    "    ratings_rdd = sc.textFile(filename)\n",
    "\n",
    "    # Remove the header from the RDD and parse each line into a tuple\n",
    "    header = ratings_rdd.first()\n",
    "    ratings_rdd = (ratings_rdd.filter(lambda line: line != header)\n",
    "                   .map(lambda line: line.split(','))\n",
    "                   .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]), tokens[3])))\n",
    "\n",
    "    num_ratings = ratings_rdd.count()\n",
    "    print(f\"Number of ratings: {num_ratings}\")\n",
    "\n",
    "    # Convert RDD to DataFrame for easier processing\n",
    "    ratings_df = spark.createDataFrame(ratings_rdd, [\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
    "\n",
    "    return spark, ratings_df\n",
    "\n",
    "\n",
    "spark, ratings_df = init_spark_session_with_ratings(\"dataset/ratings.csv\")\n",
    "print(ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 2_2: Basic Recommend with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:48.683341Z",
     "start_time": "2023-11-18T04:44:16.313299Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def basic_recommend(spark, ratings_df):\n",
    "    \"\"\"\n",
    "    Performs basic movie recommendation based on the average rating.\n",
    "\n",
    "    Args:\n",
    "    ratings_df (DataFrame): The DataFrame containing movie ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the average rating for each movie\n",
    "    avg_ratings_df = ratings_df.groupBy(\"movieId\").avg(\"rating\")\n",
    "\n",
    "    # Retrieve top 5 movies based on average ratings\n",
    "    top_movies = avg_ratings_df.orderBy(\"avg(rating)\", ascending=False).limit(5)\n",
    "    top_movies.show()\n",
    "\n",
    "    # Plotting the top 5 movies and save the figure to a file\n",
    "    top_movies_pd = top_movies.toPandas()   # Convert Spark DataFrame to Pandas DataFrame for plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Adding width to the bars and making them visually distinct\n",
    "    plt.bar(top_movies_pd['movieId'].astype(str), top_movies_pd['avg(rating)'], width=0.5, color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "    plt.xlabel('Movie ID')\n",
    "    plt.ylabel('Average Rating')\n",
    "    plt.title('Top 5 Movies by Average Rating')\n",
    "    plt.ylim(4, 5.1)  # Ensuring that the top of the bars are within the visible range of the plot\n",
    "    plt.show()\n",
    "    plt.close()  # Close the plt object to free memory\n",
    "\n",
    "basic_recommend(spark, ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part3: ALS Recommend with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:16.141700Z",
     "start_time": "2023-11-18T04:44:50.822066Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 16) / 21]\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[Stage 1399:>                                                     (0 + 17) / 21]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/CineSpark-Insights/experiment.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m     plt\u001b[39m.\u001b[39mclose()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m als_recommend(spark, ratings_df)\n",
      "\u001b[1;32m/root/CineSpark-Insights/experiment.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m als\u001b[39m.\u001b[39msetParams(rank\u001b[39m=\u001b[39mrank, maxIter\u001b[39m=\u001b[39mmax_iter, regParam\u001b[39m=\u001b[39mreg_param)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Fit ALS model on training data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m model \u001b[39m=\u001b[39m als\u001b[39m.\u001b[39mfit(training)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Evaluate the model on test data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtransform(test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj\u001b[39m.\u001b[39mfit(dataset\u001b[39m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1399:===========================================>          (17 + 4) / 21]\r"
     ]
    }
   ],
   "source": [
    "def als_recommend(spark, ratings_df):\n",
    "    \"\"\"\n",
    "    Performs movie recommendations using the ALS (Alternating Least Squares).\n",
    "\n",
    "    Args:\n",
    "    spark (SparkSession): SparkSession object for DataFrame operations.\n",
    "    ratings_df (DataFrame): The DataFrame containing movie ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the dataset into training and test sets\n",
    "    (training, test) = ratings_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "    # Create an ALS model\n",
    "    als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\", nonnegative=True)\n",
    "\n",
    "    # Define a grid of parameters for tuning\n",
    "    param_grid = {\n",
    "        \"rank\": [10, 20],\n",
    "        \"maxIter\": [5, 10],\n",
    "        \"regParam\": [0.01, 0.1]\n",
    "    }\n",
    "\n",
    "    # Define multiple evaluators\n",
    "    rmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "    # Variables to keep track of the best model and its performance\n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    best_error = float('inf')\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    # Grid search through the parameter space\n",
    "    for rank in param_grid[\"rank\"]:\n",
    "        for max_iter in param_grid[\"maxIter\"]:\n",
    "            for reg_param in param_grid[\"regParam\"]:\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Set model parameters\n",
    "                als.setParams(rank=rank, maxIter=max_iter, regParam=reg_param)\n",
    "                \n",
    "                # Fit ALS model on training data\n",
    "                model = als.fit(training)\n",
    "                \n",
    "                # Evaluate the model on test data\n",
    "                predictions = model.transform(test)\n",
    "                rmse = rmse_evaluator.evaluate(predictions)\n",
    "                mae = mae_evaluator.evaluate(predictions)\n",
    "                training_time = time.time() - start_time\n",
    "\n",
    "                # Append results\n",
    "                results.append({\n",
    "                    \"rank\": rank, \"maxIter\": max_iter, \"regParam\": reg_param,\n",
    "                    \"RMSE\": rmse, \"MAE\": mae, \"Training Time\": training_time\n",
    "                })\n",
    "\n",
    "                # Update best model\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = model\n",
    "                    best_params = (rank, max_iter, reg_param)\n",
    "\n",
    "    # Save the results DataFrame to a CSV file\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "\n",
    "    # Display the best model parameters and its RMSE\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best RMSE:\", best_error)\n",
    "\n",
    "    # Generate top 5 movie recommendations for each user using the best model\n",
    "    recommendations = best_model.recommendForAllUsers(5)\n",
    "    recommendations.show(truncate=False)\n",
    "\n",
    "    # Explode the recommendations to create a row for each movie\n",
    "    recs_exploded = recommendations.withColumn(\"rec_exp\", explode(\"recommendations\")).select(\"userId\", col(\"rec_exp.movieId\"), col(\"rec_exp.rating\"))\n",
    "\n",
    "    # Plotting the number of times each movie is recommended\n",
    "    recs_pd = recs_exploded.toPandas()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    recs_pd['movieId'].value_counts().head(5).plot(kind='bar')\n",
    "    plt.xlabel('Movie ID')\n",
    "    plt.ylabel('Number of Recommendations')\n",
    "    plt.title('Top 5 Recommended Movies')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "als_recommend(spark, ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_recommend_best(spark, ratings_df):\n",
    "    \"\"\"\n",
    "    Performs movie recommendations using the ALS (Alternating Least Squares) model with optimal parameters.\n",
    "\n",
    "    Args:\n",
    "    spark (SparkSession): SparkSession object for DataFrame operations.\n",
    "    ratings_df (DataFrame): The DataFrame containing movie ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the dataset into training and test sets\n",
    "    (training, test) = ratings_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "    # Define multiple evaluators\n",
    "    rmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "    # Create an ALS model\n",
    "    als = ALS(rank=10,maxIter=10, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "   \n",
    "    # Fit ALS model on training data\n",
    "    model = als.fit(training)\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    predictions = model.transform(test)\n",
    "    rmse = rmse_evaluator.evaluate(predictions)\n",
    "    mae = mae_evaluator.evaluate(predictions)\n",
    "    print(f\"Root Mean Square Error (RMSE): {rmse}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "    # Generate top 5 movie recommendations for each user using the best model\n",
    "    recommendations = model.recommendForAllUsers(5)\n",
    "    recommendations.show(truncate=False)\n",
    "\n",
    "    # Prepare data for visualization\n",
    "    recs_exploded = recommendations.withColumn(\"rec_exp\", explode(\"recommendations\")).select(\"userId\", col(\"rec_exp.movieId\"), col(\"rec_exp.rating\"))\n",
    "    movies_df = spark.read.csv(\"dataset/movies.csv\", header=True, inferSchema=True)    # Movies: movieId, title, genres\n",
    "    recs_joined = recs_exploded.join(movies_df, \"movieId\").select(\"userId\", \"title\", \"rating\")\n",
    "    recs_pd = recs_joined.toPandas()\n",
    "\n",
    "    # Count the number of recommendations for each movie and get the top 5\n",
    "    top_movies = recs_pd['title'].value_counts().head(5)\n",
    "\n",
    "    # Plotting the number of times each movie is recommended using the movie titles\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_movies.plot(kind='bar')\n",
    "    plt.xlabel('Movie Title')  # Changed from 'Movie ID' to 'Movie Title'\n",
    "    plt.ylabel('Number of Recommendations')\n",
    "    plt.title('Top 5 Recommended Movies by ALS Model')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust the layout to fit the labels\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "als_recommend_best(spark, ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 4: RF Recommend with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(spark, ratings_df):\n",
    "    \"\"\"\n",
    "    Perform feature engineering for movie recommendations. This includes processing movie genres and tags,\n",
    "    and applying one-hot encoding.\n",
    "\n",
    "    Args:\n",
    "    spark (SparkSession): SparkSession object for DataFrame operations.\n",
    "    ratings_df (DataFrame): DataFrame containing movie ratings.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with combined movie features ready for model training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Converts a list of vectors into a single vector by summing up each dimension.\n",
    "    def sum_vectors(vectors):\n",
    "        return Vectors.dense(sum(v[0] for v in vectors))\n",
    "\n",
    "    sum_vectors_udf = udf(sum_vectors, VectorUDT())\n",
    "\n",
    "    # Load and preprocess datasets\n",
    "    movies_df = spark.read.csv(\"dataset/movies.csv\", header=True, inferSchema=True)    # movieId, title, genres\n",
    "    tags_df = spark.read.csv(\"dataset/tags.csv\", header=True, inferSchema=True)    # userId, movieId, tag, timestamp\n",
    "    print(\"Initial Movies DataFrame:\", movies_df.take(5))\n",
    "    print(\"Initial Tags DataFrame:\", tags_df.take(5))\n",
    "\n",
    "    # Process genres: split, explode, and one-hot encode\n",
    "    movies_df = movies_df.withColumn(\"split_genres\", split(col(\"genres\"), \"\\|\"))    # Split genres into individual genres\n",
    "    movies_exploded = movies_df.withColumn(\"genre\", explode(col(\"split_genres\")))   # Explode genres into new rows\n",
    "    genre_indexer = StringIndexer(inputCol=\"genre\", outputCol=\"genreIndex\")\n",
    "    indexed_genre = genre_indexer.fit(movies_exploded).transform(movies_exploded)\n",
    "    genre_encoder = OneHotEncoder(inputCol=\"genreIndex\", outputCol=\"genreVec\")\n",
    "    encoded_genre = genre_encoder.fit(indexed_genre).transform(indexed_genre)\n",
    "    print(\"Movies DataFrame after splitting genres:\", movies_df.take(5))\n",
    "    print(\"Movies DataFrame after splitting and exploding genres:\", movies_exploded.take(5))\n",
    "    print(\"Indexed Genre DataFrame:\", indexed_genre.take(5))\n",
    "    print(\"Encoded Genre DataFrame:\", encoded_genre.take(5))\n",
    "\n",
    "    # Aggregate the encoded genres back to movie level\n",
    "    genre_aggregated = encoded_genre.groupBy(\"movieId\").agg(collect_list(\"genreVec\").alias(\"genreVecList\"))\n",
    "    genre_aggregated = genre_aggregated.withColumn(\"genresVec\", sum_vectors_udf(\"genreVecList\"))\n",
    "    print(\"Aggregated Genre DataFrame:\", genre_aggregated.take(5))\n",
    "\n",
    "    # Process tags using StringIndexer + OneHotEncoder\n",
    "    tag_indexer = StringIndexer(inputCol=\"tag\", outputCol=\"tagIndex\")\n",
    "    tag_model = tag_indexer.fit(tags_df)\n",
    "    indexed_tags = tag_model.transform(tags_df)\n",
    "    tag_encoder = OneHotEncoder(inputCols=[\"tagIndex\"], outputCols=[\"tagVec\"])\n",
    "    tags_encoded = tag_encoder.fit(indexed_tags).transform(indexed_tags)\n",
    "    print(\"Indexed Tags DataFrame:\", indexed_tags.take(5))\n",
    "    print(\"Encoded Tags DataFrame:\", tags_encoded.take(5))\n",
    "\n",
    "    # Add a new column 'label' based on the condition\n",
    "    ratings_df = ratings_df.withColumn('label', F.when(F.col('rating') >= 3.5, 2).otherwise(3))\n",
    "\n",
    "    def parse_year(title):\n",
    "    pattern = r\"\\((\\d{4})\\)\"  # Regular expression pattern to extract the year\n",
    "    match = re.search(pattern, title)\n",
    "    if match:\n",
    "        year_str = match.group(1)\n",
    "        return int(year_str)\n",
    "    else:\n",
    "        return 1900\n",
    "     # Extract movie year\n",
    "    parse_year_udf = udf(parse_year, IntegerType())\n",
    "    complete_data_df = complete_data_df.withColumn(\"movie_year\", parse_year_udf(complete_data_df['title']))\n",
    "\n",
    "    # Explode tag vectors into a list of features for each movie\n",
    "    movie_tags_features = tags_encoded.groupBy('movieId').agg(F.collect_list('tagVec').alias('tagVectors'))\n",
    "    movie_tags_features = movie_tags_features.withColumn('tagFeatures', sum_vectors_udf('tagVectors')).drop('tagVectors')\n",
    "    print(\"Movie Tags Features DataFrame:\", movie_tags_features.take(5))\n",
    "\n",
    "    # Combine movie features with ratings\n",
    "    complete_data_df = ratings_df.join(genre_aggregated.select(\"movieId\", \"genresVec\"), \"movieId\").join(movie_tags_features, \"movieId\")\n",
    "    print(\"Complete Data DataFrame after combining features with ratings:\", complete_data_df.take(5))\n",
    "\n",
    "    # Create feature vectors\n",
    "    assembler = VectorAssembler(\n",
    "    inputCols=[\"userId\", \"movieId\", \"timestamp\", \"movie_year\", \"label\"] + [\"genresVec\", \"tagFeatures\"],\n",
    "    outputCol=\"features\"\n",
    "    )    \n",
    "    data_ready = assembler.transform(complete_data_df)\n",
    "\n",
    "    return data_ready\n",
    "\n",
    "# Assuming spark and ratings_df are defined earlier in your code\n",
    "data_ready = feature_engineering(spark, ratings_df)\n",
    "print(data_ready)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_visualize(data_ready):\n",
    "    \"\"\"\n",
    "    Train, evaluate, and visualize movie recommendations using a Random Forest model.\n",
    "\n",
    "    Args:\n",
    "    data_ready (DataFrame): DataFrame containing prepared features for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Splitting the dataset\n",
    "    training_features, test_features = data_ready.randomSplit([0.7, 0.3])\n",
    "\n",
    "    # Training the Random Forest model\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"rating\")\n",
    "    model = rf.fit(training_features)\n",
    "\n",
    "    # Evaluate model on test dataset\n",
    "    predictions_df = model.transform(test_features)\n",
    "    rmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "    # Calculate and output evaluation metrics\n",
    "    rmse = rmse_evaluator.evaluate(predictions_df)\n",
    "    mae = mae_evaluator.evaluate(predictions_df)\n",
    "    print(f\"Root Mean Square Error (RMSE): {rmse}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "    #Calculate average predition rating of every movie\n",
    "    avg_predictions_df = predictions_df.groupBy(\"movieId\").avg(\"prediction\")\n",
    "\n",
    "    # Get top5 movie recommendations\n",
    "    top_predictions = predictions_df.orderBy('avg(prediction)', ascending=False).limit(5)\n",
    "\n",
    "    # Join with the movies dataframe to get movie titles\n",
    "    movies_df = spark.read.csv(\"dataset/movies.csv\", header=True, inferSchema=True)  # Load movies data\n",
    "    top_movies_with_titles = top_predictions.join(movies_df, 'movieId').select('title', 'avg(prediction)')\n",
    "    print(top_movies_with_titles)\n",
    "\n",
    "    # Convert to pandas dataframe for visualization\n",
    "    top_movies_pd = top_movies_with_titles.toPandas()\n",
    "    top_movies_pd.set_index('title', inplace=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_movies_pd['avg(prediction)'].plot(kind='bar')\n",
    "    plt.xlabel('Predicted Rating')\n",
    "    plt.ylabel('Movie Title')\n",
    "    plt.title('Top 5 Movie Recommendations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "train_evaluate_visualize(data_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 5: Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**ALS model**:\n",
    "+ Advantages: It is suitable for large-scale data sets, can effectively handle sparsity problems, and is often used in recommendation systems.\n",
    "+ Disadvantages: Need to adjust multiple parameters, sensitive to cold start issues.\n",
    "\n",
    "**Random forest model**:\n",
    "+ Advantages: It handles nonlinear relationships well and is less prone to overfitting.\n",
    "+ Disadvantages: A large amount of feature engineering is required and the computational cost is high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
