{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 1: Basic Data Manipulation & Simple Recommendation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.Read in the rating file and create an RDD consisting of parsed lines, then count the number of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:13.970800Z",
     "start_time": "2023-11-18T04:43:57.362997Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a Spark session\n",
    "sc = SparkContext .getOrCreate()\n",
    "\n",
    "ratings_rdd = sc.textFile(\"dataset/ratings.csv\")\n",
    "header = ratings_rdd.first()\n",
    "\n",
    "# Remove the header and then parse each line\n",
    "ratings_rdd = ratings_rdd.filter(lambda line: line != header) \\\n",
    "    .map(lambda line: line.split(',')) \\\n",
    "    .map(lambda tokens: (tokens[0], tokens[1], float(tokens[2]), tokens[3]))\n",
    "\n",
    "num_ratings = ratings_rdd.count()\n",
    "print(num_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Recommend 5 movies with the highest average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:48.683341Z",
     "start_time": "2023-11-18T04:44:16.313299Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===================================================>     (19 + 2) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|movieId|avg(rating)|\n",
      "+-------+-----------+\n",
      "| 195549|        5.0|\n",
      "| 169820|        5.0|\n",
      "| 182345|        5.0|\n",
      "| 137964|        5.0|\n",
      "| 159048|        5.0|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MovieLens\").config(\"spark.driver.memory\", \"8G\").getOrCreate()\n",
    "ratings_df = spark.createDataFrame(ratings_rdd, [\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
    "\n",
    "#  Count the number of ratings\n",
    "avg_ratings_df = ratings_df.groupBy(\"movieId\").avg(\"rating\")\n",
    "\n",
    "# Recommend 5 movies with the highest average rating\n",
    "top_movies = avg_ratings_df.orderBy(\"avg(rating)\", ascending=False).limit(5)\n",
    "\n",
    "top_movies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Other operations to enrich your data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:44:48.688111Z",
     "start_time": "2023-11-18T04:44:48.686403Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Try to create visualizations to convey the insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:16.141700Z",
     "start_time": "2023-11-18T04:44:50.822066Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve the data in the form of a Pandas DataFrame\n",
    "top_movies_pd = top_movies.toPandas()\n",
    "\n",
    "# Create a visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_movies_pd['movieId'], top_movies_pd['avg(rating)'])\n",
    "plt.xlabel('Movie ID')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Top 5 Movies by Average Rating')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part2: Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:26:27.728586Z",
     "start_time": "2023-11-18T04:26:27.034583Z"
    },
    "collapsed": false
   },
   "source": [
    "## 1. First split rating data into 70% training set and 30% testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-18T04:45:24.738544Z",
     "start_time": "2023-11-18T04:45:20.115597Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 08:54:35 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 6 (TID 52): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "ratings_ml = ratings_df.rdd.map(lambda r: Row(userId=int(r[0]), movieId=int(r[1]), rating=float(r[2])))\n",
    "ratings_ml_df = spark.createDataFrame(ratings_ml)\n",
    "\n",
    "# Split the dataset\n",
    "(training, test) = ratings_ml_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Choose one matrix factorization algorithm to predict the rating score based on the rating data file only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 05:14:00 WARN BlockManager: Block rdd_49_0 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:00 WARN BlockManager: Block rdd_50_0 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:00 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 156)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3284)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "23/11/23 05:14:01 WARN BlockManager: Block rdd_49_3 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:01 WARN BlockManager: Block rdd_50_3 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:01 ERROR Executor: Exception in task 3.0 in stage 16.0 (TID 159)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3356)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:471)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.result(ArrayBuilder.scala:515)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.result(ArrayBuilder.scala:462)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "23/11/23 05:14:01 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 16.0 (TID 159),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3356)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:471)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.result(ArrayBuilder.scala:515)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.result(ArrayBuilder.scala:462)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "23/11/23 05:14:01 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 16.0 (TID 156),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3284)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "23/11/23 05:14:01 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 156) (192.168.102.123 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3284)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\n",
      "23/11/23 05:14:01 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job\n",
      "23/11/23 05:14:01 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 156) (192.168.102.123 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3284)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1266)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:995)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:737)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:714)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:616)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3284)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\n",
      "23/11/23 05:14:01 WARN TaskSetManager: Lost task 9.0 in stage 16.0 (TID 165) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 05:14:03 WARN BlockManager: Putting block rdd_49_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:03 WARN BlockManager: Block rdd_49_8 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:03 WARN BlockManager: Putting block rdd_50_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:03 WARN BlockManager: Block rdd_50_8 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:03 WARN TaskSetManager: Lost task 8.0 in stage 16.0 (TID 164) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "[Stage 16:>                                                        (0 + 6) / 10]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o127.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 156) (192.168.102.123 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3284)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1266)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:995)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:737)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:714)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:616)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3284)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/root/CineSpark-Insights/experiment.ipynb 单元格 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m als\u001b[39m.\u001b[39msetParams(rank\u001b[39m=\u001b[39mrank, maxIter\u001b[39m=\u001b[39mmax_iter, regParam\u001b[39m=\u001b[39mreg_param)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Fit ALS model on training data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m model \u001b[39m=\u001b[39m als\u001b[39m.\u001b[39;49mfit(training)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Evaluate on test data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.123/root/CineSpark-Insights/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtransform(test)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o127.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 156) (192.168.102.123 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3284)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1266)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:995)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:737)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:714)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:616)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3284)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:339)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:383)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.result(ArrayBuilder.scala:330)\n\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.build(ALS.scala:1452)\n\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1660)\n\tat org.apache.spark.ml.recommendation.ALS$$$Lambda$3655/451741565.apply(Unknown Source)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:752)\n\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda$3636/1159170700.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:377)\n\tat org.apache.spark.rdd.RDD$$Lambda$3591/389148629.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\n\tat org.apache.spark.storage.BlockManager$$Lambda$911/1756540820.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 05:14:20 WARN BlockManager: Block rdd_49_4 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:20 WARN BlockManager: Block rdd_50_4 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:20 WARN BlockManager: Asked to remove block rdd_49_4, which does not exist\n",
      "23/11/23 05:14:20 ERROR Executor: Exception in task 4.0 in stage 16.0 (TID 160)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/11/23 05:14:20 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 16.0 (TID 160),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/11/23 05:14:22 WARN BlockManager: Putting block rdd_49_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:22 WARN BlockManager: Block rdd_49_5 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:22 WARN BlockManager: Putting block rdd_50_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:22 WARN BlockManager: Block rdd_50_5 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:22 WARN TaskSetManager: Lost task 5.0 in stage 16.0 (TID 161) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 05:14:22 WARN BlockManager: Putting block rdd_49_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:22 WARN BlockManager: Block rdd_49_1 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:22 WARN BlockManager: Putting block rdd_50_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:22 WARN BlockManager: Block rdd_50_1 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:22 WARN TaskSetManager: Lost task 1.0 in stage 16.0 (TID 157) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 05:14:22 WARN BlockManager: Putting block rdd_49_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:22 WARN BlockManager: Block rdd_49_6 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:22 WARN BlockManager: Putting block rdd_50_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:22 WARN BlockManager: Block rdd_50_6 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:22 WARN TaskSetManager: Lost task 6.0 in stage 16.0 (TID 162) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 05:14:22 WARN BlockManager: Putting block rdd_49_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:22 WARN BlockManager: Block rdd_49_2 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:22 WARN BlockManager: Asked to remove block rdd_49_2, which does not exist\n",
      "23/11/23 05:14:22 WARN BlockManager: Putting block rdd_50_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:22 WARN BlockManager: Block rdd_50_2 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:22 WARN TaskSetManager: Lost task 2.0 in stage 16.0 (TID 158) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "23/11/23 05:14:22 WARN BlockManager: Putting block rdd_49_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:22 WARN BlockManager: Block rdd_49_7 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:22 WARN BlockManager: Asked to remove block rdd_49_7, which does not exist\n",
      "23/11/23 05:14:22 WARN BlockManager: Putting block rdd_50_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "23/11/23 05:14:22 WARN BlockManager: Block rdd_50_7 could not be removed as it was not found on disk or in memory\n",
      "23/11/23 05:14:22 WARN TaskSetManager: Lost task 7.0 in stage 16.0 (TID 163) (192.168.102.123 executor driver): TaskKilled (Stage cancelled)\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 38610)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/root/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "\n",
    "# Create ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\", nonnegative=True)\n",
    "\n",
    "# Define a grid of parameters to search\n",
    "param_grid = {\n",
    "    \"rank\": [10, 20, 30],\n",
    "    \"maxIter\": [5, 10, 20],\n",
    "    \"regParam\": [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "best_model = None\n",
    "best_error = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Grid search through parameters\n",
    "for rank in param_grid[\"rank\"]:\n",
    "    for max_iter in param_grid[\"maxIter\"]:\n",
    "        for reg_param in param_grid[\"regParam\"]:\n",
    "            als.setParams(rank=rank, maxIter=max_iter, regParam=reg_param)\n",
    "            \n",
    "            # Fit ALS model on training data\n",
    "            model = als.fit(training)\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            predictions = model.transform(test)\n",
    "            error = evaluator.evaluate(predictions)\n",
    "            \n",
    "            # Update best parameters if current model is better\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_model = model\n",
    "                best_params = {\"rank\": rank, \"maxIter\": max_iter, \"regParam\": reg_param}\n",
    "\n",
    "# Output the best parameters and error\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best RMSE:\", best_error)\n",
    "\n",
    "# Make predictions using the best model\n",
    "test_predictions = best_model.transform(test)\n",
    "\n",
    "# Generate top 3 recommendations for all users using the best model\n",
    "recommendations = best_model.recommendForAllUsers(3)\n",
    "recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on the test data = {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Extract features from movies and users (join movie and user data and do some feature transformation), then build another machine learning model to predict rating scores for the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.1 Read and integrate additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read additional data\n",
    "tags_df = spark.read.csv(\"dataset/tags.csv\", header=True, inferSchema=True)\n",
    "movies_df = spark.read.csv(\"dataset/movies.csv\", header=True, inferSchema=True)\n",
    "genome_scores_df = spark.read.csv(\"dataset/genome-scores.csv\", header=True, inferSchema=True)\n",
    "genome_tags_df = spark.read.csv(\"dataset/genome-tags.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Potential feature transformations\n",
    "# For example, perform one-hot encoding on movie genres\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Type conversion\n",
    "stringIndexer = StringIndexer(inputCol=\"genres\", outputCol=\"genresIndex\")\n",
    "model = stringIndexer.fit(movies_df)\n",
    "indexed = model.transform(movies_df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"genresIndex\", outputCol=\"genresVec\")\n",
    "movies_encoded = encoder.transform(indexed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 合并标签和评分数据\n",
    "# 这里是一个简化的例子，具体实现可能更复杂\n",
    "tag_features_df = tags_df.join(genome_scores_df, \"movieId\").join(genome_tags_df, \"tagId\")\n",
    "\n",
    "# 将电影信息和标签特征合并\n",
    "movie_features_df = movies_encoded.join(tag_features_df, \"movieId\")\n",
    "\n",
    "# 合并用户评分和电影特征\n",
    "complete_data_df = ratings_ml_df.join(movie_features_df, \"movieId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 Build and train machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# 特征向量化\n",
    "assembler = VectorAssembler(inputCols=[\"genresVec\", \"tagFeatures\"], outputCol=\"features\")\n",
    "data_ready = assembler.transform(complete_data_df)\n",
    "\n",
    "# 划分数据集\n",
    "(training_features, test_features) = data_ready.randomSplit([0.7, 0.3])\n",
    "\n",
    "# 使用随机森林模型\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"rating\")\n",
    "rf_model = rf.fit(training_features)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions_rf = rf_model.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Compare the pros and cons of these two models and report it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ALS模型:\n",
    "+ 优点：适合大规模数据集，能有效处理稀疏性问题，常用于推荐系统。\n",
    "+ 缺点：需要调整多个参数，对冷启动问题敏感。\n",
    "\n",
    "随机森林模型:\n",
    "+ 优点：处理非线性关系效果好，不太容易过拟合。\n",
    "+ 缺点：需要大量特征工程，计算成本较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. Try to create visualizations to convey the insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算误差\n",
    "predictions_pd = predictions.toPandas()\n",
    "predictions_rf_pd = predictions_rf.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(predictions_pd['rating'] - predictions_pd['prediction'], bins=20, color='blue', alpha=0.7)\n",
    "plt.title('ALS Prediction Error')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions_rf_pd['rating'] - predictions_rf_pd['prediction'], bins=20, color='green', alpha=0.7)\n",
    "plt.title('Random Forest Prediction Error')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
