{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.Read in the rating file and create an RDD consisting of parsed lines, then count the number of ratings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==============================================>          (17 + 4) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# 获取当前活跃的 SparkContext 或创建一个新的\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "ratings_rdd = sc.textFile(\"../dataset/ratings.csv\")\n",
    "header = ratings_rdd.first()\n",
    "\n",
    "# 去除头部，然后解析每一行\n",
    "ratings_rdd = ratings_rdd.filter(lambda line: line != header) \\\n",
    "    .map(lambda line: line.split(',')) \\\n",
    "    .map(lambda tokens: (tokens[0], tokens[1], float(tokens[2]), tokens[3]))\n",
    "\n",
    "num_ratings = ratings_rdd.count()\n",
    "print(num_ratings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T04:21:59.906001Z",
     "start_time": "2023-11-18T04:21:50.798642Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Recommend 5 movies with the highest average rating."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|movieId|avg(rating)|\n",
      "+-------+-----------+\n",
      "| 195549|        5.0|\n",
      "| 140014|        5.0|\n",
      "| 133297|        5.0|\n",
      "| 196547|        5.0|\n",
      "| 182345|        5.0|\n",
      "+-------+-----------+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MovieLens\").getOrCreate()\n",
    "ratings_df = spark.createDataFrame(ratings_rdd, [\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
    "\n",
    "# 计算每部电影的平均评分\n",
    "avg_ratings_df = ratings_df.groupBy(\"movieId\").avg(\"rating\")\n",
    "\n",
    "# 降序排列并选出前5部\n",
    "top_movies = avg_ratings_df.orderBy(\"avg(rating)\", ascending=False).limit(5)\n",
    "\n",
    "top_movies.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T04:22:30.885164Z",
     "start_time": "2023-11-18T04:22:01.734511Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Other operations to enrich your data analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Try to create visualizations to convey the insights."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 以Pandas DataFrame的形式获取数据\n",
    "top_movies_pd = top_movies.toPandas()\n",
    "\n",
    "# 创建可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_movies_pd['movieId'], top_movies_pd['avg(rating)'])\n",
    "plt.xlabel('Movie ID')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Top 5 Movies by Average Rating')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T04:26:34.008174Z",
     "start_time": "2023-11-18T04:26:33.998082Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. First split rating data into 70% training set and 30% testing set."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T04:26:27.728586Z",
     "start_time": "2023-11-18T04:26:27.034583Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "ratings_ml = ratings_df.rdd.map(lambda r: Row(userId=int(r[0]), movieId=int(r[1]), rating=float(r[2])))\n",
    "ratings_ml_df = spark.createDataFrame(ratings_ml)\n",
    "\n",
    "# 划分数据集\n",
    "(training, test) = ratings_ml_df.randomSplit([0.7, 0.3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Choose one matrix factorization algorithm to predict the rating score based on the rating data file only."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# 使用MLlib中的ALS（交替最小二乘法）算法进行评分预测是一个常见选择。\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\", nonnegative=True)\n",
    "model = als.fit(training)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = model.transform(test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Extract features from movies and users (join movie and user data and do some feature transformation), then build another machine learning model to predict rating scores for the testing set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Read and integrate additional data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 读取额外的数据\n",
    "tags_df = spark.read.csv(\"dataset/tags.csv\", header=True, inferSchema=True)\n",
    "movies_df = spark.read.csv(\"dataset/movies.csv\", header=True, inferSchema=True)\n",
    "genome_scores_df = spark.read.csv(\"dataset/genome-scores.csv\", header=True, inferSchema=True)\n",
    "genome_tags_df = spark.read.csv(\"dataset/genome-tags.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 可能的特征转换\n",
    "# 例如，对电影类型进行独热编码\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# 类型转换\n",
    "stringIndexer = StringIndexer(inputCol=\"genres\", outputCol=\"genresIndex\")\n",
    "model = stringIndexer.fit(movies_df)\n",
    "indexed = model.transform(movies_df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"genresIndex\", outputCol=\"genresVec\")\n",
    "movies_encoded = encoder.transform(indexed)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 合并标签和评分数据\n",
    "# 这里是一个简化的例子，具体实现可能更复杂\n",
    "tag_features_df = tags_df.join(genome_scores_df, \"movieId\").join(genome_tags_df, \"tagId\")\n",
    "\n",
    "# 将电影信息和标签特征合并\n",
    "movie_features_df = movies_encoded.join(tag_features_df, \"movieId\")\n",
    "\n",
    "# 合并用户评分和电影特征\n",
    "complete_data_df = ratings_ml_df.join(movie_features_df, \"movieId\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Build and train machine learning models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# 特征向量化\n",
    "assembler = VectorAssembler(inputCols=[\"genresVec\", \"tagFeatures\"], outputCol=\"features\")\n",
    "data_ready = assembler.transform(complete_data_df)\n",
    "\n",
    "# 划分数据集\n",
    "(training_features, test_features) = data_ready.randomSplit([0.7, 0.3])\n",
    "\n",
    "# 使用随机森林模型\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"rating\")\n",
    "rf_model = rf.fit(training_features)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions_rf = rf_model.transform(test_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Compare the pros and cons of these two models and report it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ALS模型:\n",
    "+ 优点：适合大规模数据集，能有效处理稀疏性问题，常用于推荐系统。\n",
    "+ 缺点：需要调整多个参数，对冷启动问题敏感。\n",
    "\n",
    "随机森林模型:\n",
    "+ 优点：处理非线性关系效果好，不太容易过拟合。\n",
    "+ 缺点：需要大量特征工程，计算成本较高。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Try to create visualizations to convey the insights."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算误差\n",
    "predictions_pd = predictions.toPandas()\n",
    "predictions_rf_pd = predictions_rf.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(predictions_pd['rating'] - predictions_pd['prediction'], bins=20, color='blue', alpha=0.7)\n",
    "plt.title('ALS Prediction Error')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions_rf_pd['rating'] - predictions_rf_pd['prediction'], bins=20, color='green', alpha=0.7)\n",
    "plt.title('Random Forest Prediction Error')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
