{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# 创建SparkSession\n",
    "spark = SparkSession.builder.appName(\"MovieRecommend\")\\\n",
    "    .config(\"spark.executor.memory\", \"8G\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .config(\"spark.executor.instances\", \"4\")\\\n",
    "    .config(\"spark.driver.memory\", \"8G\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "ratings_df = spark.read.csv(\"dataset/ratings.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# Read additional data\n",
    "tags_df = spark.read.csv(\"dataset/tags.csv\", header=True, inferSchema=True)\n",
    "movies_df = spark.read.csv(\"dataset/movies.csv\", header=True, inferSchema=True)\n",
    "genome_scores_df = spark.read.csv(\"dataset/genome-scores.csv\", header=True, inferSchema=True)\n",
    "genome_tags_df = spark.read.csv(\"dataset/genome-tags.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join DataFrames\n",
    "# complete_data = movies_df.join(tags_df, \"movieId\") \\\n",
    "#                         .join(genome_scores_df, \"movieId\") \\\n",
    "#                         .join(genome_tags_df, \"tagId\") \\\n",
    "#                         .join(ratings_df, [\"userId\", \"movieId\"])\n",
    "\n",
    "# genome_data=genome_scores_df.join(genome_tags_df,\"tagId\")\n",
    "\n",
    "# complete_data=ratings_df.join(movies_df,\"movieId\")\\\n",
    "# .join(tags_df,\"movieId\")\\\n",
    "# .join(genome_data,[\"movieId\",\"tag\"])\n",
    "\n",
    "# complete_data.printSchema()\n",
    "# # complete_data.show(5)\n",
    "# complete_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def add_sample_label(ratings_df):\n",
    "    # Show the first 5 rows and print the schema before modification\n",
    "    ratings_df.show(5, truncate=False)\n",
    "    ratings_df.printSchema()\n",
    "\n",
    "    # Add a new column 'label' based on the condition\n",
    "    ratings_df = ratings_df.withColumn('label', F.when(F.col('rating') >= 3.5, 2).otherwise(3))\n",
    "\n",
    "    return ratings_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Extract movie year\n",
    "def parse_year(title):\n",
    "    pattern = r\"\\((\\d{4})\\)\"  # 正则表达式模式，匹配括号内的四位数字\n",
    "    match = re.search(pattern, title)\n",
    "    if match:\n",
    "        year_str = match.group(1)\n",
    "        return int(year_str)\n",
    "    else:\n",
    "        return 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import os\n",
    "import pickle\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# # spark = SparkSession.builder.appName(\"PrepareData\").getOrCreate()\n",
    "# # 创建SparkSession\n",
    "# spark = SparkSession.builder.appName(\"MovieRecommend\")\\\n",
    "#     .config(\"spark.executor.memory\", \"8G\")\\\n",
    "#     .config(\"spark.executor.cores\", \"4\")\\\n",
    "#     .config(\"spark.executor.instances\", \"4\")\\\n",
    "#     .config(\"spark.driver.memory\", \"8G\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "\n",
    "# Read movie data\n",
    "data_path = \"dataset/\"\n",
    "df_movie = spark.read.csv(\n",
    "    \"dataset/movies.csv\", header=True,inferSchema=True)\n",
    "\n",
    "# Extract movie year\n",
    "parse_year_udf = udf(parse_year, IntegerType())\n",
    "df_movie = df_movie.withColumn(\"movie_year\", parse_year_udf(df_movie['title']))\n",
    "df_movie=df_movie.drop('title')\n",
    "\n",
    "# # Join genome data to df\n",
    "# df = df.join(genome_scores_df, \"movieId\")\n",
    "# df = df.join(genome_tags_df, \"tagId\")\n",
    "\n",
    "# One-hot encoding genres\n",
    "genres = df_movie.select(\"genres\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "genres_unique = list(set([i for sublist in genres for i in sublist.split(\"|\")]))\n",
    "\n",
    "for genres_name in genres_unique:\n",
    "    col = \"genres_\" + genres_name\n",
    "    df_movie = df_movie.withColumn(col, \n",
    "                                   (df_movie.genres.contains(genres_name)).cast(IntegerType()))\n",
    "\n",
    "df_movie = df_movie.drop('genres')\n",
    "genres_col_names = [\"genres_\" + x for x in genres_unique]\n",
    "\n",
    "# Read rating data\n",
    "\n",
    "df_rating = spark.read.csv(\n",
    "    \"dataset/ratings.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_rating = add_sample_label(df_rating)\n",
    "df_rating = df_rating.withColumn('timestamp', (df_rating['timestamp'] / (365 * 24 * 3600)).cast(IntegerType()))\n",
    "\n",
    "# Data merge\n",
    "df = df_rating.join(df_movie, 'movieId')\n",
    "\n",
    "\n",
    "# df_X = df.select([\"userId\", \"movieId\"] + genres_col_names + [\"timestamp\"])\n",
    "# df_y = df.select([\"rating\"])\n",
    "# print(\"Data read completed\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(20)\n",
    "# df_X.show(10)\n",
    "# df_y.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# 使用 VectorAssembler 进行特征组合\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"userId\", \"movieId\", \"timestamp\",\"movie_year\",\"label\"] + genres_col_names ,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# 对数据框进行转换\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# 打印数据读取完成的消息\n",
    "print(\"Data read completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# # Create a Spark session\n",
    "# spark = SparkSession.builder.appName(\"RandomForestRegressorExample\").getOrCreate()\n",
    "\n",
    "\n",
    "# Select features and target column\n",
    "df = df.select(\"features\", \"rating\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(training_data, test_data) = df.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "# Create a RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"rating\")\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.maxBins, [16, 32, 64]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "# Define multiple evaluators\n",
    "rmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Create the CrossValidator\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=rmse_evaluator,\n",
    "                          numFolds=3)  # Number of folds for cross-validation\n",
    "\n",
    "# Record the start time\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters\n",
    "cv_model = crossval.fit(training_data)\n",
    "\n",
    "# Record the end time\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Calculate the training time\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training Time: {training_time}\")\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions_rf_cv = cv_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_cv = rmse_evaluator.evaluate(predictions_rf_cv)\n",
    "mae_cv = mae_evaluator.evaluate(predictions_rf_cv)\n",
    "print(f\"Cross-validated Root Mean Square Error (RMSE): {rmse_cv}\")\n",
    "print(f\"Cross-validated Mean Absolute Error (MAE): {mae_cv}\")\n",
    "\n",
    "# Print the best parameters found during cross-validation\n",
    "best_params = cv_model.bestModel.extractParamMap()\n",
    "print(\"Best Parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param.name}: {value}\")\n",
    "\n",
    "# Create a pandas DataFrame with the results\n",
    "results_dict = {\n",
    "    \"Training Time\": [str(training_time)],\n",
    "    \"RMSE\": [rmse_cv],\n",
    "    \"MAE\": [mae_cv],\n",
    "    \"Best Parameters\": [best_params]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_path = \"/root/CineSpark-Insights/results/rf_model_performance.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Results saved to: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "\n",
    "# 选择特征和目标列\n",
    "df = df.select(\"features\", \"rating\")\n",
    "\n",
    "# 划分数据集为训练集和测试集\n",
    "(training_data, test_data) = df.randomSplit([0.7, 0.3],seed=123)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"rating\",\n",
    "    numTrees=10,\n",
    "    maxDepth=5,\n",
    "    maxBins=32,\n",
    "    minInstancesPerNode=1\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model_rf = rf.fit(training_data)\n",
    "\n",
    "# Predicting on test data\n",
    "predictions_rf = model_rf.transform(test_data)\n",
    "\n",
    "\n",
    "\n",
    "# Define multiple evaluators\n",
    "rmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "mae = mae_evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
