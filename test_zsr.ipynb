{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Environment Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T14:24:41.946407Z",
     "start_time": "2023-11-30T14:24:41.243023Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, SparseVector\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, collect_list, split, explode, udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Init Spark Session with Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T14:25:49.375616Z",
     "start_time": "2023-11-30T14:24:43.031196Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/30 13:27:37 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.102.123 instead (on interface ens192)\n",
      "23/11/30 13:27:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/30 13:27:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/30 13:27:38 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[userId: int, movieId: int, rating: double, timestamp: int]\n",
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    296|   5.0|1147880044|\n",
      "|     1|    306|   3.5|1147868817|\n",
      "|     1|    307|   5.0|1147868828|\n",
      "|     1|    665|   5.0|1147878820|\n",
      "|     1|    899|   3.5|1147868510|\n",
      "|     1|   1088|   4.0|1147868495|\n",
      "|     1|   1175|   3.5|1147868826|\n",
      "|     1|   1217|   3.5|1147878326|\n",
      "|     1|   1237|   5.0|1147868839|\n",
      "|     1|   1250|   4.0|1147868414|\n",
      "|     1|   1260|   3.5|1147877857|\n",
      "|     1|   1653|   4.0|1147868097|\n",
      "|     1|   2011|   2.5|1147868079|\n",
      "|     1|   2012|   2.5|1147868068|\n",
      "|     1|   2068|   2.5|1147869044|\n",
      "|     1|   2161|   3.5|1147868609|\n",
      "|     1|   2351|   4.5|1147877957|\n",
      "|     1|   2573|   4.0|1147878923|\n",
      "|     1|   2632|   5.0|1147878248|\n",
      "|     1|   2692|   5.0|1147869100|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_spark_session_with_ratings(filename):\n",
    "    \"\"\"\n",
    "    Reads the ratings file and returns the SparkSession and DataFrame created from the file.\n",
    "\n",
    "    Args:\n",
    "    filename (str): The path to the ratings file.\n",
    "\n",
    "    Returns:\n",
    "    SparkSession, DataFrame: The Spark session and the DataFrame created from the ratings file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MovieRecommend\") \\\n",
    "        .config(\"spark.local.dir\", \"/home/spark-local-dir\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.executor.instances\", \"4\") \\\n",
    "        .config(\"spark.driver.memory\", \"4G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    ratings_df = spark.read.csv(\"dataset/ratings.csv\", header=True, inferSchema=True)\n",
    "    print(ratings_df)\n",
    "    ratings_df.show()\n",
    "\n",
    "    return spark, ratings_df\n",
    "\n",
    "\n",
    "spark, ratings_df = init_spark_session_with_ratings(\"dataset/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T14:28:35.698119Z",
     "start_time": "2023-11-30T14:25:51.787453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- genresVec: vector (nullable = true)\n",
      " |-- tagVectors: array (nullable = false)\n",
      " |    |-- element: vector (containsNull = false)\n",
      " |-- tagFeatures: vector (nullable = true)\n",
      " |-- genomeFeaturesVec: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[movieId: int, userId: int, rating: double, timestamp: int, genresVec: vector, tagVectors: array<vector>, tagFeatures: vector, genomeFeaturesVec: vector, features: vector]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf, col, explode, split, collect_list\n",
    "import numpy as np\n",
    "\n",
    "def feature_engineering(spark, ratings_df):\n",
    "    # Converts a list of vectors into a single vector by summing up each dimension.\n",
    "    def sum_vectors(vectors):\n",
    "        length = max(v.size for v in vectors)\n",
    "        sum_vector = np.zeros(length)\n",
    "        for v in vectors:\n",
    "            sum_vector += v.toArray() if isinstance(v, SparseVector) else v\n",
    "        return Vectors.dense(sum_vector)\n",
    "\n",
    "    # Converts a column of movie features into a DenseVector.\n",
    "    def to_vector(col):\n",
    "        return Vectors.dense(col)\n",
    "\n",
    "    sum_vectors_udf = udf(sum_vectors, VectorUDT())\n",
    "    to_vector_udf = udf(to_vector, VectorUDT())\n",
    "\n",
    "    # Load and preprocess datasets\n",
    "    # Replace these lines with your actual data loading code\n",
    "    movies_df = spark.read.csv(\"dataset/movies.csv\", header=True, inferSchema=True)\n",
    "    tags_df = spark.read.csv(\"dataset/tags.csv\", header=True, inferSchema=True)\n",
    "    genome_scores_df = spark.read.csv(\"dataset/genome-scores.csv\", header=True, inferSchema=True)\n",
    "    genome_tags_df = spark.read.csv(\"dataset/genome-tags.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Process genome scores\n",
    "    genome_df = genome_scores_df.join(genome_tags_df, \"tagId\")\n",
    "    movie_genome_features = genome_df.groupBy('movieId').agg(collect_list('relevance').alias('genomeFeatures'))\n",
    "    # 获取基因组标签的数量\n",
    "    vector_length = genome_tags_df.count()\n",
    "\n",
    "    # 创建一个稀疏向量的UDF来填充空值\n",
    "    def fill_with_sparse_vector(v):\n",
    "        if v is None:\n",
    "            return SparseVector(vector_length, [], [])\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    # 注册UDF\n",
    "    fill_with_sparse_vector_udf = udf(fill_with_sparse_vector, VectorUDT())\n",
    "\n",
    "    # 使用withColumn和UDF来替换空值\n",
    "    movie_genome_features = movie_genome_features.withColumn(\n",
    "        'genomeFeaturesVec', \n",
    "        fill_with_sparse_vector_udf(F.col('genomeFeatures'))\n",
    "    )\n",
    "\n",
    "    # Process genres using StringIndexer + OneHotEncoder\n",
    "    movies_df = movies_df.withColumn(\"split_genres\", split(col(\"genres\"), \"\\|\"))\n",
    "    movies_exploded = movies_df.withColumn(\"genre\", explode(col(\"split_genres\")))\n",
    "    genre_indexer = StringIndexer(inputCol=\"genre\", outputCol=\"genreIndex\")\n",
    "    indexed_genre = genre_indexer.fit(movies_exploded).transform(movies_exploded)\n",
    "    genre_encoder = OneHotEncoder(inputCol=\"genreIndex\", outputCol=\"genreVec\")\n",
    "    encoded_genre = genre_encoder.fit(indexed_genre).transform(indexed_genre)\n",
    "    genre_aggregated = encoded_genre.groupBy(\"movieId\").agg(collect_list(\"genreVec\").alias(\"genreVecList\"))\n",
    "    genre_aggregated = genre_aggregated.withColumn(\"genresVec\", sum_vectors_udf(\"genreVecList\"))\n",
    "\n",
    "    # Process tags using StringIndexer + OneHotEncoder\n",
    "    tags_df = tags_df.join(genome_tags_df, tags_df.tag == genome_tags_df.tag, \"inner\").select(tags_df[\"*\"])\n",
    "    tag_indexer = StringIndexer(inputCol=\"tag\", outputCol=\"tagIndex\")\n",
    "    tag_model = tag_indexer.fit(tags_df)\n",
    "    indexed_tags = tag_model.transform(tags_df)\n",
    "    tag_encoder = OneHotEncoder(inputCols=[\"tagIndex\"], outputCols=[\"tagVec\"])\n",
    "    tags_encoded = tag_encoder.fit(indexed_tags).transform(indexed_tags)\n",
    "    movie_tags_features = tags_encoded.groupBy('movieId').agg(collect_list('tagVec').alias('tagVectors'))\n",
    "    movie_tags_features = movie_tags_features.withColumn('tagFeatures', sum_vectors_udf('tagVectors'))\n",
    "\n",
    "    movie_genome_features = movie_genome_features.withColumn('genomeFeaturesVec', to_vector_udf('genomeFeatures'))\n",
    "\n",
    "    # Combine movie features with ratings\n",
    "    complete_data_df = ratings_df.join(genre_aggregated.select(\"movieId\", \"genresVec\"), \"movieId\") \\\n",
    "        .join(movie_tags_features, \"movieId\") \\\n",
    "        .join(movie_genome_features.select('movieId', 'genomeFeaturesVec'), 'movieId', 'left')  # Use the corrected column\n",
    "\n",
    "    # 检查数据集 schema\n",
    "    complete_data_df.printSchema()\n",
    "\n",
    "    # 删除包含空值的行\n",
    "    complete_data_df = complete_data_df.dropna()\n",
    "\n",
    "    # Assemble features into a single column\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"genresVec\", \"tagFeatures\", \"genomeFeaturesVec\"], \n",
    "        outputCol=\"features\",\n",
    "    )\n",
    "    data_ready = assembler.transform(complete_data_df)\n",
    "\n",
    "    # 确保数据类型正确\n",
    "    data_ready = data_ready.withColumn(\"rating\", col(\"rating\").cast(\"double\"))\n",
    "\n",
    "    return data_ready\n",
    "\n",
    "# Assuming spark is your SparkSession and ratings_df is the DataFrame you are passing\n",
    "data_ready = feature_engineering(spark, ratings_df)\n",
    "print(data_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T14:35:51.556244Z",
     "start_time": "2023-11-30T14:29:04.753316Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "def train_evaluate_visualize(data_ready):\n",
    "    \"\"\"\n",
    "    Train, evaluate, and visualize movie recommendations using a Random Forest model.\n",
    "\n",
    "    Args:\n",
    "    data_ready (DataFrame): DataFrame containing prepared features for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Splitting the dataset\n",
    "    training_features, test_features = data_ready.randomSplit([0.7, 0.3])\n",
    "\n",
    "    # Training the Random Forest model with optimized parameters\n",
    "    rf = RandomForestRegressor(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"rating\",\n",
    "        numTrees=5,            # 减少树的数量以加快训练\n",
    "        maxDepth=5,             # 降低树的最大深度\n",
    "        maxBins=32,\n",
    "        featureSubsetStrategy=\"auto\",  # 自动选择特征子集\n",
    "        subsamplingRate=0.7,    # 在每棵树的训练中使用70%的数据\n",
    "        minInstancesPerNode=1  # 每个节点的最小实例数\n",
    "    )\n",
    "    model = rf.fit(training_features)\n",
    "\n",
    "    # Evaluate model on test dataset\n",
    "    predictions_df = model.transform(test_features)\n",
    "    rmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "    # Calculate and output evaluation metrics\n",
    "    rmse = rmse_evaluator.evaluate(predictions_df)\n",
    "    mae = mae_evaluator.evaluate(predictions_df)\n",
    "    print(f\"Root Mean Square Error (RMSE): {rmse}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "    # Get top5 movie recommendations\n",
    "    top_predictions = predictions_df.orderBy('prediction', ascending=False).limit(5)\n",
    "\n",
    "    # Join with the movies dataframe to get movie titles\n",
    "    movies_df = spark.read.csv(\"dataset/movies.csv\", header=True, inferSchema=True)  # Load movies data\n",
    "    top_movies_with_titles = top_predictions.join(movies_df, 'movieId').select('title', 'prediction')\n",
    "    print(top_movies_with_titles)\n",
    "\n",
    "\n",
    "train_evaluate_visualize(data_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 5: Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**ALS model**:\n",
    "+ Advantages: It is suitable for large-scale data sets, can effectively handle sparsity problems, and is often used in recommendation systems.\n",
    "+ Disadvantages: Need to adjust multiple parameters, sensitive to cold start issues.\n",
    "\n",
    "**Random forest model**:\n",
    "+ Advantages: It handles nonlinear relationships well and is less prone to overfitting.\n",
    "+ Disadvantages: A large amount of feature engineering is required and the computational cost is high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
